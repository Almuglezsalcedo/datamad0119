{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos nltk y confirmamos que va ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/almu/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('an', 'AT'),\n",
       " ('investigation', 'NN'),\n",
       " ('of', 'IN')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.tagged_words()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/almu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/inputtransformer2.py:468: UserWarning: `make_tokens_by_line` received a list of lines which do not have lineending markers ('\\n', '\\r', '\\r\\n', '\\x0b', '\\x0c'), behavior will be unspecified\n",
      "  warnings.warn(\"`make_tokens_by_line` received a list of lines which do not have lineending markers ('\\\\n', '\\\\r', '\\\\r\\\\n', '\\\\x0b', '\\\\x0c'), behavior will be unspecified\")\n"
     ]
    }
   ],
   "source": [
    "text = 'Ironhack is a Global Tech School ranked num 2 worldwide.  ",
    " ",
    "Our mission is to help people transform their careers and join a thriving community of tech professionals that love what they do. This ideology is reflected in our teaching practices, which consist of a nine-weeks immersive programming, UX/UI design or Data Analytics course as well as a one-week hiring fair aimed at helping our students change their career and get a job straight after the course. We are present in 8 countries and have campuses in 9 locations - Madrid, Barcelona, Miami, Paris, Mexico City,  Berlin, Amsterdam, Sao Paulo and Lisbon.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ironhack is a Global Tech School ranked num 2 worldwide.',\n",
       " 'Our mission is to help people transform their careers and join a thriving community of tech professionals that love what they do.',\n",
       " 'This ideology is reflected in our teaching practices, which consist of a nine-weeks immersive programming, UX/UI design or Data Analytics course as well as a one-week hiring fair aimed at helping our students change their career and get a job straight after the course.',\n",
       " 'We are present in 8 countries and have campuses in 9 locations - Madrid, Barcelona, Miami, Paris, Mexico City,  Berlin, Amsterdam, Sao Paulo and Lisbon.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ironhack',\n",
       " 'is',\n",
       " 'a',\n",
       " 'Global',\n",
       " 'Tech',\n",
       " 'School',\n",
       " 'ranked',\n",
       " 'num',\n",
       " '2',\n",
       " 'worldwide',\n",
       " '.',\n",
       " 'Our',\n",
       " 'mission',\n",
       " 'is',\n",
       " 'to',\n",
       " 'help',\n",
       " 'people',\n",
       " 'transform',\n",
       " 'their',\n",
       " 'careers',\n",
       " 'and',\n",
       " 'join',\n",
       " 'a',\n",
       " 'thriving',\n",
       " 'community',\n",
       " 'of',\n",
       " 'tech',\n",
       " 'professionals',\n",
       " 'that',\n",
       " 'love',\n",
       " 'what',\n",
       " 'they',\n",
       " 'do',\n",
       " '.',\n",
       " 'This',\n",
       " 'ideology',\n",
       " 'is',\n",
       " 'reflected',\n",
       " 'in',\n",
       " 'our',\n",
       " 'teaching',\n",
       " 'practices',\n",
       " ',',\n",
       " 'which',\n",
       " 'consist',\n",
       " 'of',\n",
       " 'a',\n",
       " 'nine-weeks',\n",
       " 'immersive',\n",
       " 'programming',\n",
       " ',',\n",
       " 'UX/UI',\n",
       " 'design',\n",
       " 'or',\n",
       " 'Data',\n",
       " 'Analytics',\n",
       " 'course',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'a',\n",
       " 'one-week',\n",
       " 'hiring',\n",
       " 'fair',\n",
       " 'aimed',\n",
       " 'at',\n",
       " 'helping',\n",
       " 'our',\n",
       " 'students',\n",
       " 'change',\n",
       " 'their',\n",
       " 'career',\n",
       " 'and',\n",
       " 'get',\n",
       " 'a',\n",
       " 'job',\n",
       " 'straight',\n",
       " 'after',\n",
       " 'the',\n",
       " 'course',\n",
       " '.',\n",
       " 'We',\n",
       " 'are',\n",
       " 'present',\n",
       " 'in',\n",
       " '8',\n",
       " 'countries',\n",
       " 'and',\n",
       " 'have',\n",
       " 'campuses',\n",
       " 'in',\n",
       " '9',\n",
       " 'locations',\n",
       " '-',\n",
       " 'Madrid',\n",
       " ',',\n",
       " 'Barcelona',\n",
       " ',',\n",
       " 'Miami',\n",
       " ',',\n",
       " 'Paris',\n",
       " ',',\n",
       " 'Mexico',\n",
       " 'City',\n",
       " ',',\n",
       " 'Berlin',\n",
       " ',',\n",
       " 'Amsterdam',\n",
       " ',',\n",
       " 'Sao',\n",
       " 'Paulo',\n",
       " 'and',\n",
       " 'Lisbon',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase='''@Ironhack's-#Q website 776-is http://ironhack.com [(2018)]\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ironhack s  q website     is           '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def clean_up(s):\n",
    "    lines = s.lower().split(\" \")\n",
    "    for i in lines:\n",
    "        if i.startswith('http'):\n",
    "            lines.remove(i)\n",
    "    lines = [re.sub(r'\\d', ' ', line) for line in lines]\n",
    "    lines = [re.sub(r'\\W', ' ', line) for line in lines]\n",
    "    \n",
    "    return \" \".join(lines)\n",
    "  \n",
    "    \n",
    "phrase_cleaned=clean_up(phrase)\n",
    "phrase_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ironhack', 's', 'q', 'website', 'is']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(s):\n",
    "    return word_tokenize(s)\n",
    "phrase_tokenized=tokenize(phrase_cleaned)\n",
    "phrase_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/almu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Error loading stem: Package 'stem' not found in index\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stem')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import stem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ironhack', 's', 'q', 'websit', 'is']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem_and_lemmatize(l):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = stem.PorterStemmer()\n",
    "    stem_phrase=[stemmer.stem(x) for x in l]\n",
    "    stem_and_lemmatize_phrase = [lemmatizer.lemmatize(x)  for x in stem_phrase] \n",
    "    return stem_and_lemmatize_phrase\n",
    "lemm_phrase=stem_and_lemmatize(phrase_tokenized)\n",
    "lemm_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ironhack', 's', 'q', 'websit']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stop_words import get_stop_words\n",
    "def remove_stopwords(w):\n",
    "    stop_words = get_stop_words('en')\n",
    "    [w.remove(i) for i in w if i in stop_words]\n",
    "    return w\n",
    "remove_stopwords(lemm_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "zf = zipfile.ZipFile('./Sentiment140.csv.zip') \n",
    "df = pd.read_csv(zf.open('Sentiment140.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsam = df.sample(n=20000, random_state=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_f(x): \n",
    "    functions = [clean_up, tokenize, stem_and_lemmatize, remove_stopwords]\n",
    "    for f in functions: \n",
    "        x = f(x)\n",
    "    return x\n",
    "dfsam['text_processed']=dfsam.text.apply(clean_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>text_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>514293</th>\n",
       "      <td>0</td>\n",
       "      <td>2190584004</td>\n",
       "      <td>Tue Jun 16 03:08:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Vicki_Gee</td>\n",
       "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
       "      <td>[miss, nikki, nu, nu, alreadi, alway, when, ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142282</th>\n",
       "      <td>0</td>\n",
       "      <td>1881451988</td>\n",
       "      <td>Fri May 22 04:42:15 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>PatCashin</td>\n",
       "      <td>So I had a dream last night. I  remember a sig...</td>\n",
       "      <td>[dream, last, night, rememb, sign, clearli, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403727</th>\n",
       "      <td>0</td>\n",
       "      <td>2058252964</td>\n",
       "      <td>Sat Jun 06 14:34:17 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>deelectable</td>\n",
       "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
       "      <td>[girlyghost, ohh, poor, sickli, hug, hope, fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649503</th>\n",
       "      <td>0</td>\n",
       "      <td>2237307600</td>\n",
       "      <td>Fri Jun 19 05:34:22 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>justinekepa</td>\n",
       "      <td>it is raining again</td>\n",
       "      <td>[is, rain]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610789</th>\n",
       "      <td>0</td>\n",
       "      <td>2224301193</td>\n",
       "      <td>Thu Jun 18 09:20:06 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>cmatt007</td>\n",
       "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
       "      <td>[misskeribabi, wish, wa, la, right, now]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        target          id                          date      flag  \\\n",
       "514293       0  2190584004  Tue Jun 16 03:08:48 PDT 2009  NO_QUERY   \n",
       "142282       0  1881451988  Fri May 22 04:42:15 PDT 2009  NO_QUERY   \n",
       "403727       0  2058252964  Sat Jun 06 14:34:17 PDT 2009  NO_QUERY   \n",
       "649503       0  2237307600  Fri Jun 19 05:34:22 PDT 2009  NO_QUERY   \n",
       "610789       0  2224301193  Thu Jun 18 09:20:06 PDT 2009  NO_QUERY   \n",
       "\n",
       "               user                                               text  \\\n",
       "514293    Vicki_Gee  i miss nikki nu nu already  shes always there ...   \n",
       "142282    PatCashin  So I had a dream last night. I  remember a sig...   \n",
       "403727  deelectable  @girlyghost ohh poor sickly you   (((hugs)) ho...   \n",
       "649503  justinekepa                               it is raining again    \n",
       "610789     cmatt007          @MissKeriBaby wish I was in LA right now    \n",
       "\n",
       "                                           text_processed  \n",
       "514293  [miss, nikki, nu, nu, alreadi, alway, when, ne...  \n",
       "142282  [dream, last, night, rememb, sign, clearli, to...  \n",
       "403727  [girlyghost, ohh, poor, sickli, hug, hope, fee...  \n",
       "649503                                         [is, rain]  \n",
       "610789           [misskeribabi, wish, wa, la, right, now]  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import ConditionalFreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 3369,\n",
       " 's': 2463,\n",
       " 't': 2252,\n",
       " 'i': 2162,\n",
       " 'a': 2057,\n",
       " 'go': 1718,\n",
       " 'm': 1702,\n",
       " 'just': 1593,\n",
       " 'my': 1440,\n",
       " 'get': 1385,\n",
       " 'wa': 1334,\n",
       " 'day': 1317,\n",
       " 'now': 1248,\n",
       " 'thi': 1206,\n",
       " 'good': 1158,\n",
       " 'can': 1143,\n",
       " 'like': 1057,\n",
       " 'love': 1039,\n",
       " 'work': 1026,\n",
       " 'to': 1000,\n",
       " 'you': 992,\n",
       " 'it': 916,\n",
       " 'got': 894,\n",
       " 'quot': 871,\n",
       " 'u': 863,\n",
       " 'have': 827,\n",
       " 'time': 793,\n",
       " 'today': 778,\n",
       " 'miss': 753,\n",
       " 'want': 750,\n",
       " 'lol': 745,\n",
       " 'back': 734,\n",
       " 'thank': 731,\n",
       " 'one': 692,\n",
       " 'will': 664,\n",
       " 'realli': 664,\n",
       " 'don': 660,\n",
       " 'know': 657,\n",
       " 'im': 632,\n",
       " 'think': 625,\n",
       " 'see': 614,\n",
       " 'amp': 614,\n",
       " 'feel': 602,\n",
       " 'watch': 585,\n",
       " 'be': 582,\n",
       " 'need': 572,\n",
       " 'still': 556,\n",
       " 'well': 549,\n",
       " 'night': 545,\n",
       " 'make': 542,\n",
       " 'hope': 539,\n",
       " 'oh': 529,\n",
       " 'home': 525,\n",
       " 'll': 519,\n",
       " 'look': 515,\n",
       " 'na': 512,\n",
       " 'new': 512,\n",
       " 'ha': 490,\n",
       " 'is': 473,\n",
       " 'me': 468,\n",
       " 'come': 465,\n",
       " 'twitter': 458,\n",
       " 'much': 457,\n",
       " 'last': 453,\n",
       " 'am': 442,\n",
       " 'that': 438,\n",
       " 'wish': 426,\n",
       " 'morn': 426,\n",
       " 'in': 423,\n",
       " 'great': 418,\n",
       " 'wait': 416,\n",
       " 'do': 408,\n",
       " 'are': 407,\n",
       " 're': 404,\n",
       " 'so': 403,\n",
       " 'sad': 392,\n",
       " 'tomorrow': 390,\n",
       " 'haha': 379,\n",
       " 'sleep': 376,\n",
       " 'right': 371,\n",
       " 'and': 365,\n",
       " 'whi': 361,\n",
       " 'all': 356,\n",
       " 'fun': 348,\n",
       " 'thing': 347,\n",
       " 'tonight': 345,\n",
       " 'follow': 345,\n",
       " 'onli': 343,\n",
       " 'happi': 342,\n",
       " 'friend': 341,\n",
       " 'week': 337,\n",
       " 'nice': 336,\n",
       " 'tri': 332,\n",
       " 'bad': 330,\n",
       " 'veri': 330,\n",
       " 'on': 329,\n",
       " 'sorri': 325,\n",
       " 've': 324,\n",
       " 'not': 322,\n",
       " 'hi': 322,\n",
       " 'take': 314,\n",
       " 'say': 307,\n",
       " 'd': 303,\n",
       " 'way': 290,\n",
       " 'better': 288,\n",
       " 'school': 283,\n",
       " 'yeah': 277,\n",
       " 'for': 276,\n",
       " 'hate': 275,\n",
       " 'bed': 269,\n",
       " 'start': 267,\n",
       " 'tweet': 267,\n",
       " 'peopl': 267,\n",
       " 'gon': 265,\n",
       " 'of': 264,\n",
       " 'though': 261,\n",
       " 'hour': 261,\n",
       " 'show': 258,\n",
       " 'even': 254,\n",
       " 'your': 254,\n",
       " 'guy': 251,\n",
       " 'weekend': 248,\n",
       " 'play': 239,\n",
       " 'everyon': 235,\n",
       " 'let': 234,\n",
       " 'littl': 233,\n",
       " 'final': 232,\n",
       " 'cant': 232,\n",
       " 'had': 230,\n",
       " 'life': 230,\n",
       " 'lt': 230,\n",
       " 'too': 229,\n",
       " 'didn': 228,\n",
       " 'hey': 226,\n",
       " 'ye': 226,\n",
       " 'use': 223,\n",
       " 'wan': 222,\n",
       " 'movi': 221,\n",
       " 'year': 219,\n",
       " 'rain': 218,\n",
       " 'first': 218,\n",
       " 'sick': 216,\n",
       " 'would': 216,\n",
       " 'ok': 215,\n",
       " 'awesom': 210,\n",
       " 'tire': 209,\n",
       " 'find': 206,\n",
       " 'best': 204,\n",
       " 'never': 204,\n",
       " 'next': 203,\n",
       " 'soon': 202,\n",
       " 'there': 202,\n",
       " 'ani': 200,\n",
       " 'no': 198,\n",
       " 'done': 198,\n",
       " 'talk': 198,\n",
       " 'call': 197,\n",
       " 'head': 197,\n",
       " 'man': 197,\n",
       " 'sure': 197,\n",
       " 'phone': 196,\n",
       " 'dont': 195,\n",
       " 'anoth': 192,\n",
       " 'with': 191,\n",
       " 'plea': 190,\n",
       " 'help': 190,\n",
       " 'alreadi': 189,\n",
       " 'some': 189,\n",
       " 'x': 189,\n",
       " 'alway': 188,\n",
       " 'cool': 188,\n",
       " 'long': 187,\n",
       " 'mayb': 187,\n",
       " 'but': 186,\n",
       " 'keep': 185,\n",
       " 'hurt': 185,\n",
       " 'yay': 185,\n",
       " 'could': 184,\n",
       " 'leav': 183,\n",
       " 'song': 182,\n",
       " 'lot': 181,\n",
       " 'they': 180,\n",
       " 'we': 179,\n",
       " 'girl': 177,\n",
       " 'made': 176,\n",
       " 'someth': 176,\n",
       " 'at': 175,\n",
       " 'enjoy': 175,\n",
       " 'live': 173,\n",
       " 'eat': 173,\n",
       " 'bore': 173,\n",
       " 'won': 172,\n",
       " 'thought': 171,\n",
       " 'old': 171,\n",
       " 'yet': 171,\n",
       " 'becaus': 171,\n",
       " 'read': 170,\n",
       " 'suck': 170,\n",
       " 'ever': 169,\n",
       " 'ur': 167,\n",
       " 'readi': 164,\n",
       " 'pretti': 164,\n",
       " 'lost': 163,\n",
       " 'actual': 162,\n",
       " 'finish': 162,\n",
       " 'away': 162,\n",
       " 'sound': 160,\n",
       " 'went': 160,\n",
       " 'should': 158,\n",
       " 'an': 158,\n",
       " 'hous': 158,\n",
       " 'excit': 157,\n",
       " 'n': 155,\n",
       " 'listen': 155,\n",
       " 'hear': 154,\n",
       " 'summer': 153,\n",
       " 'happen': 151,\n",
       " 'someon': 151,\n",
       " 'game': 151,\n",
       " 'left': 151,\n",
       " 'guess': 147,\n",
       " 'befor': 147,\n",
       " 'noth': 147,\n",
       " 'w': 146,\n",
       " 'earli': 146,\n",
       " 'wonder': 145,\n",
       " 'doe': 145,\n",
       " 'up': 144,\n",
       " 'job': 143,\n",
       " 'tell': 143,\n",
       " 'sinc': 143,\n",
       " 'babi': 143,\n",
       " 'her': 142,\n",
       " 'late': 142,\n",
       " 'omg': 141,\n",
       " 'glad': 140,\n",
       " 'he': 139,\n",
       " 'end': 139,\n",
       " 'ugh': 137,\n",
       " 'bit': 137,\n",
       " 'car': 137,\n",
       " 'gone': 137,\n",
       " 'weather': 137,\n",
       " 'saw': 136,\n",
       " 'beauti': 136,\n",
       " 'kid': 136,\n",
       " 'o': 136,\n",
       " 'pic': 135,\n",
       " 'ya': 134,\n",
       " 'wow': 134,\n",
       " 'check': 134,\n",
       " 'mom': 134,\n",
       " 'later': 133,\n",
       " 'damn': 133,\n",
       " 'here': 132,\n",
       " 'amaz': 132,\n",
       " 'birthday': 132,\n",
       " 'did': 131,\n",
       " 'then': 130,\n",
       " 'stop': 129,\n",
       " 'b': 129,\n",
       " 'mean': 129,\n",
       " 'big': 128,\n",
       " 'p': 127,\n",
       " 'hot': 127,\n",
       " 'parti': 127,\n",
       " 'give': 127,\n",
       " 'out': 127,\n",
       " 'said': 126,\n",
       " 'also': 126,\n",
       " 'sun': 125,\n",
       " 'god': 124,\n",
       " 'die': 124,\n",
       " 'two': 124,\n",
       " 'run': 123,\n",
       " 'may': 123,\n",
       " 'updat': 123,\n",
       " 'r': 123,\n",
       " 'com': 122,\n",
       " 'must': 121,\n",
       " 'what': 121,\n",
       " 'more': 119,\n",
       " 'move': 119,\n",
       " 'shit': 119,\n",
       " 'seem': 118,\n",
       " 'world': 118,\n",
       " 'hard': 117,\n",
       " 'iphon': 117,\n",
       " 'put': 117,\n",
       " 'haven': 117,\n",
       " 'ta': 116,\n",
       " 'free': 116,\n",
       " 'cold': 115,\n",
       " 'yesterday': 114,\n",
       " 'luck': 112,\n",
       " 'meet': 112,\n",
       " 'might': 112,\n",
       " 'month': 111,\n",
       " 'studi': 111,\n",
       " 'stay': 111,\n",
       " 'found': 111,\n",
       " 'mine': 111,\n",
       " 'th': 110,\n",
       " 'aww': 109,\n",
       " 'gt': 108,\n",
       " 'music': 108,\n",
       " 'mani': 108,\n",
       " 'ill': 108,\n",
       " 'friday': 107,\n",
       " 'lunch': 107,\n",
       " 'buy': 106,\n",
       " 'fuck': 106,\n",
       " 'woke': 106,\n",
       " 'doesn': 105,\n",
       " 'book': 105,\n",
       " 'exam': 105,\n",
       " 'about': 105,\n",
       " 'shop': 105,\n",
       " 'boy': 104,\n",
       " 'post': 104,\n",
       " 'around': 103,\n",
       " 'clean': 103,\n",
       " 'monday': 103,\n",
       " 'tho': 103,\n",
       " 'cri': 103,\n",
       " 'least': 102,\n",
       " 'video': 102,\n",
       " 'were': 102,\n",
       " 'she': 101,\n",
       " 'stuff': 101,\n",
       " 'okay': 101,\n",
       " 'famili': 100,\n",
       " 'till': 99,\n",
       " 'anyth': 99,\n",
       " 'welcom': 99,\n",
       " 'pictur': 97,\n",
       " 'forward': 97,\n",
       " 'sooo': 97,\n",
       " 'drink': 97,\n",
       " 'busi': 96,\n",
       " 'everyth': 96,\n",
       " 'believ': 96,\n",
       " 'hahaha': 96,\n",
       " 'cute': 95,\n",
       " 'drive': 95,\n",
       " 'sweet': 95,\n",
       " 'food': 95,\n",
       " 'train': 94,\n",
       " 'xx': 94,\n",
       " 'walk': 94,\n",
       " 'everi': 94,\n",
       " 'stupid': 94,\n",
       " 'anyon': 94,\n",
       " 'isn': 93,\n",
       " 'sit': 92,\n",
       " 'sunday': 92,\n",
       " 'probabl': 92,\n",
       " 'poor': 91,\n",
       " 'turn': 91,\n",
       " 'outsid': 91,\n",
       " 'el': 91,\n",
       " 'win': 91,\n",
       " 'plan': 90,\n",
       " 'name': 90,\n",
       " 'chang': 90,\n",
       " 'almost': 89,\n",
       " 'far': 89,\n",
       " 'dad': 89,\n",
       " 'write': 89,\n",
       " 'place': 88,\n",
       " 'our': 88,\n",
       " 'hair': 88,\n",
       " 'goodnight': 88,\n",
       " 'wrong': 88,\n",
       " 'dream': 87,\n",
       " 'e': 87,\n",
       " 'ask': 87,\n",
       " 'caus': 86,\n",
       " 'kill': 86,\n",
       " 'rememb': 85,\n",
       " 'tv': 85,\n",
       " 'fan': 85,\n",
       " 'blog': 85,\n",
       " 'how': 85,\n",
       " 'dog': 84,\n",
       " 'real': 84,\n",
       " 'rest': 84,\n",
       " 'funni': 84,\n",
       " 'other': 84,\n",
       " 'repli': 83,\n",
       " 'quit': 83,\n",
       " 'face': 83,\n",
       " 'wake': 83,\n",
       " 'total': 83,\n",
       " 'if': 82,\n",
       " 'class': 82,\n",
       " 'minut': 81,\n",
       " 'worri': 81,\n",
       " 'room': 81,\n",
       " 'eye': 80,\n",
       " 'c': 80,\n",
       " 'anymor': 80,\n",
       " 'hit': 79,\n",
       " 'came': 79,\n",
       " 'been': 78,\n",
       " 'over': 78,\n",
       " 'true': 78,\n",
       " 'dinner': 78,\n",
       " 'without': 78,\n",
       " 'seen': 78,\n",
       " 'send': 78,\n",
       " 'offic': 78,\n",
       " 'mother': 77,\n",
       " 'word': 77,\n",
       " 'sister': 77,\n",
       " 'whole': 77,\n",
       " 'vote': 77,\n",
       " 'hang': 77,\n",
       " 'money': 77,\n",
       " 'aw': 76,\n",
       " 'link': 76,\n",
       " 'news': 76,\n",
       " 'brother': 76,\n",
       " 'l': 76,\n",
       " 'onc': 75,\n",
       " 'either': 75,\n",
       " 'danc': 75,\n",
       " 'when': 74,\n",
       " 'pain': 74,\n",
       " 'break': 74,\n",
       " 'person': 74,\n",
       " 'open': 74,\n",
       " 'took': 74,\n",
       " 'idea': 74,\n",
       " 'k': 73,\n",
       " 'headach': 73,\n",
       " 'coffe': 73,\n",
       " 'y': 73,\n",
       " 'saturday': 71,\n",
       " 'www': 71,\n",
       " 'photo': 71,\n",
       " 'hehe': 71,\n",
       " 'hug': 70,\n",
       " 'awww': 70,\n",
       " 'hello': 70,\n",
       " 'wasn': 70,\n",
       " 'kinda': 70,\n",
       " 'close': 70,\n",
       " 'onlin': 70,\n",
       " 'half': 70,\n",
       " 'couldn': 69,\n",
       " 'bring': 69,\n",
       " 'anyway': 69,\n",
       " 'ah': 69,\n",
       " 'text': 69,\n",
       " 'st': 68,\n",
       " 'abl': 68,\n",
       " 'crap': 68,\n",
       " 'enough': 68,\n",
       " 'off': 68,\n",
       " 'set': 68,\n",
       " 'care': 68,\n",
       " 'them': 68,\n",
       " 'trip': 68,\n",
       " 'reason': 68,\n",
       " 'full': 67,\n",
       " 'dude': 67,\n",
       " 'crazi': 67,\n",
       " 'jealou': 67,\n",
       " 'cours': 67,\n",
       " 'fall': 66,\n",
       " 'comput': 65,\n",
       " 'fix': 65,\n",
       " 'season': 64,\n",
       " 'heard': 64,\n",
       " 'same': 64,\n",
       " 'fine': 64,\n",
       " 'interest': 63,\n",
       " 'g': 63,\n",
       " 'pm': 63,\n",
       " 'bought': 63,\n",
       " 'favorit': 63,\n",
       " 'wont': 62,\n",
       " 'forgot': 62,\n",
       " 'didnt': 62,\n",
       " 'kind': 62,\n",
       " 'add': 62,\n",
       " 'site': 61,\n",
       " 'again': 61,\n",
       " 'rock': 61,\n",
       " 'high': 61,\n",
       " 'beach': 60,\n",
       " 'visit': 60,\n",
       " 'pay': 60,\n",
       " 'awak': 60,\n",
       " 'sunni': 59,\n",
       " 'relax': 59,\n",
       " 'problem': 59,\n",
       " 'learn': 59,\n",
       " 'dead': 59,\n",
       " 'sign': 58,\n",
       " 'heart': 58,\n",
       " 'super': 58,\n",
       " 'serious': 58,\n",
       " 'ago': 58,\n",
       " 'red': 58,\n",
       " 'cuz': 58,\n",
       " 'fail': 57,\n",
       " 'afternoon': 57,\n",
       " 'star': 57,\n",
       " 'soo': 57,\n",
       " 'asleep': 57,\n",
       " 'drop': 57,\n",
       " 'smile': 57,\n",
       " 'mileycyru': 57,\n",
       " 'few': 57,\n",
       " 'mind': 56,\n",
       " 'line': 56,\n",
       " 'lucki': 56,\n",
       " 'hell': 56,\n",
       " 'email': 56,\n",
       " 'sore': 56,\n",
       " 'tommcfli': 56,\n",
       " 'congrat': 56,\n",
       " 'la': 55,\n",
       " 'ride': 55,\n",
       " 'math': 55,\n",
       " 'lose': 55,\n",
       " 'part': 55,\n",
       " 'him': 55,\n",
       " 'definit': 55,\n",
       " 'concert': 55,\n",
       " 'min': 55,\n",
       " 'boo': 55,\n",
       " 'instead': 55,\n",
       " 'broke': 55,\n",
       " 'wear': 55,\n",
       " 'power': 55,\n",
       " 'mr': 55,\n",
       " 'dear': 54,\n",
       " 'offici': 54,\n",
       " 'facebook': 54,\n",
       " 'ticket': 54,\n",
       " 'those': 54,\n",
       " 'shower': 54,\n",
       " 'hand': 54,\n",
       " 'nd': 53,\n",
       " 'mad': 53,\n",
       " 'mention': 53,\n",
       " 'youtub': 53,\n",
       " 'breakfast': 53,\n",
       " 'short': 53,\n",
       " 'from': 53,\n",
       " 'ladi': 53,\n",
       " 'perfect': 53,\n",
       " 'internet': 53,\n",
       " 'btw': 53,\n",
       " 'sometim': 53,\n",
       " 'order': 53,\n",
       " 'bet': 52,\n",
       " 'agre': 52,\n",
       " 'cat': 52,\n",
       " 'v': 52,\n",
       " 'award': 51,\n",
       " 'gym': 51,\n",
       " 'catch': 51,\n",
       " 'togeth': 51,\n",
       " 'test': 51,\n",
       " 'team': 51,\n",
       " 'beat': 51,\n",
       " 'suppos': 51,\n",
       " 'figur': 51,\n",
       " 'sat': 51,\n",
       " 'park': 51,\n",
       " 'homework': 50,\n",
       " 'pack': 50,\n",
       " 'june': 50,\n",
       " 'upload': 50,\n",
       " 'alon': 50,\n",
       " 'soooo': 50,\n",
       " 'pick': 50,\n",
       " 'pas': 50,\n",
       " 'store': 50,\n",
       " 'nap': 50,\n",
       " 'xxx': 49,\n",
       " 'second': 49,\n",
       " 'stuck': 49,\n",
       " 'dress': 49,\n",
       " 'yea': 49,\n",
       " 'by': 49,\n",
       " 'sing': 49,\n",
       " 'lmao': 49,\n",
       " 'nite': 49,\n",
       " 'xd': 49,\n",
       " 'album': 48,\n",
       " 'jona': 48,\n",
       " 'coupl': 48,\n",
       " 'side': 48,\n",
       " 'water': 48,\n",
       " 'hungri': 48,\n",
       " 'wed': 48,\n",
       " 'sigh': 48,\n",
       " 'goin': 47,\n",
       " 'vacat': 47,\n",
       " 'air': 47,\n",
       " 'easi': 47,\n",
       " 'em': 47,\n",
       " 'laptop': 47,\n",
       " 'cousin': 47,\n",
       " 'ice': 46,\n",
       " 'bless': 46,\n",
       " 'foot': 46,\n",
       " 'decid': 46,\n",
       " 'save': 46,\n",
       " 'account': 46,\n",
       " 'cook': 46,\n",
       " 'holiday': 46,\n",
       " 'wors': 45,\n",
       " 'le': 45,\n",
       " 'til': 45,\n",
       " 'revis': 45,\n",
       " 'citi': 45,\n",
       " 'or': 45,\n",
       " 'join': 45,\n",
       " 'past': 45,\n",
       " 'spend': 45,\n",
       " 'goe': 44,\n",
       " 'point': 44,\n",
       " 'top': 44,\n",
       " 'sleepi': 44,\n",
       " 'f': 44,\n",
       " 'moment': 44,\n",
       " 'window': 44,\n",
       " 'stori': 44,\n",
       " 'ad': 44,\n",
       " 'graduat': 44,\n",
       " 'differ': 44,\n",
       " 'mood': 44,\n",
       " 'answer': 43,\n",
       " 'ipod': 43,\n",
       " 'current': 43,\n",
       " 'forget': 43,\n",
       " 'understand': 43,\n",
       " 'yep': 43,\n",
       " 'throat': 43,\n",
       " 'chanc': 43,\n",
       " 'tour': 43,\n",
       " 'town': 43,\n",
       " 'fast': 43,\n",
       " 'hmm': 43,\n",
       " 'told': 42,\n",
       " 'load': 42,\n",
       " 'tea': 42,\n",
       " 'celebr': 42,\n",
       " 'bout': 42,\n",
       " 'mtv': 42,\n",
       " 'cream': 42,\n",
       " 'chat': 42,\n",
       " 'realiz': 42,\n",
       " 'annoy': 42,\n",
       " 'age': 42,\n",
       " 'mac': 42,\n",
       " 'knew': 42,\n",
       " 'road': 42,\n",
       " 'thursday': 42,\n",
       " 'date': 42,\n",
       " 'jonasbroth': 41,\n",
       " 'shoot': 41,\n",
       " 'expect': 41,\n",
       " 'fb': 41,\n",
       " 'camp': 41,\n",
       " 'scare': 41,\n",
       " 'cut': 41,\n",
       " 'co': 41,\n",
       " 'h': 41,\n",
       " 'ddlovato': 41,\n",
       " 'shirt': 41,\n",
       " 'via': 41,\n",
       " 'episod': 41,\n",
       " 'fli': 41,\n",
       " 'comment': 40,\n",
       " 'ahhh': 40,\n",
       " 'complet': 40,\n",
       " 'count': 40,\n",
       " 'lazi': 40,\n",
       " 'cancel': 40,\n",
       " 'ive': 40,\n",
       " 'especi': 40,\n",
       " 'card': 40,\n",
       " 'their': 40,\n",
       " 'flight': 40,\n",
       " 'ppl': 40,\n",
       " 'mum': 40,\n",
       " 'number': 40,\n",
       " 'rather': 40,\n",
       " 'download': 40,\n",
       " 'list': 40,\n",
       " 'chocol': 39,\n",
       " 'yr': 39,\n",
       " 'shoe': 39,\n",
       " 'down': 39,\n",
       " 'kick': 39,\n",
       " 'appl': 39,\n",
       " 'ate': 39,\n",
       " 'colleg': 39,\n",
       " 'sent': 39,\n",
       " 'black': 39,\n",
       " 'film': 39,\n",
       " 'state': 39,\n",
       " 'bye': 39,\n",
       " 'worst': 39,\n",
       " 'wtf': 39,\n",
       " 'wine': 38,\n",
       " 'question': 38,\n",
       " 'present': 38,\n",
       " 'laugh': 38,\n",
       " 'meant': 38,\n",
       " 'english': 38,\n",
       " 'manag': 38,\n",
       " 'share': 38,\n",
       " 'lil': 38,\n",
       " 'followfriday': 38,\n",
       " 'servic': 38,\n",
       " 'pizza': 38,\n",
       " 'worth': 38,\n",
       " 'than': 37,\n",
       " 'search': 37,\n",
       " 'ach': 37,\n",
       " 'flu': 37,\n",
       " 'juli': 37,\n",
       " 'depress': 37,\n",
       " 'nope': 37,\n",
       " 'freak': 37,\n",
       " 'smell': 37,\n",
       " 'unfortun': 37,\n",
       " 'due': 37,\n",
       " 'mess': 37,\n",
       " 'woman': 37,\n",
       " 'beer': 37,\n",
       " 'messag': 37,\n",
       " 'support': 37,\n",
       " 'most': 37,\n",
       " 'leg': 36,\n",
       " 'websit': 36,\n",
       " 'bike': 36,\n",
       " 'case': 36,\n",
       " 'touch': 36,\n",
       " 'parent': 36,\n",
       " 'church': 36,\n",
       " 'swim': 36,\n",
       " 'spent': 36,\n",
       " 'possibl': 36,\n",
       " 'boyfriend': 36,\n",
       " 'stomach': 36,\n",
       " 'yummi': 36,\n",
       " 'bitch': 36,\n",
       " 'burn': 36,\n",
       " 'hmmm': 35,\n",
       " 'lay': 35,\n",
       " 'weird': 35,\n",
       " 'bum': 35,\n",
       " 'appar': 35,\n",
       " 'becom': 35,\n",
       " 'father': 35,\n",
       " 'cake': 35,\n",
       " 'hill': 35,\n",
       " 'moon': 35,\n",
       " 'airport': 35,\n",
       " 'safe': 35,\n",
       " 'miley': 35,\n",
       " 'fell': 35,\n",
       " 'pool': 35,\n",
       " 'shame': 35,\n",
       " 'page': 35,\n",
       " 'usual': 35,\n",
       " 'shot': 35,\n",
       " 'london': 35,\n",
       " 'insid': 35,\n",
       " 'surpris': 35,\n",
       " 'round': 35,\n",
       " 'tummi': 34,\n",
       " 'j': 34,\n",
       " 'voic': 34,\n",
       " 'googl': 34,\n",
       " 'cheer': 34,\n",
       " 'broken': 34,\n",
       " 'sunshin': 34,\n",
       " 'event': 34,\n",
       " 'practic': 34,\n",
       " 'type': 34,\n",
       " 'near': 34,\n",
       " 'after': 34,\n",
       " 'hr': 34,\n",
       " 'xoxo': 34,\n",
       " 'stress': 34,\n",
       " 'huge': 34,\n",
       " 'taken': 34,\n",
       " 'warm': 33,\n",
       " 'normal': 33,\n",
       " 'alright': 33,\n",
       " 'idk': 33,\n",
       " 'own': 33,\n",
       " 'hold': 33,\n",
       " 'scari': 33,\n",
       " 'horribl': 33,\n",
       " 'stand': 33,\n",
       " 'chill': 33,\n",
       " 'block': 33,\n",
       " 'note': 33,\n",
       " 'doctor': 33,\n",
       " 'box': 33,\n",
       " 'pray': 33,\n",
       " 'ball': 33,\n",
       " 'cup': 32,\n",
       " 'sooooo': 32,\n",
       " 'absolut': 32,\n",
       " 'slow': 32,\n",
       " 'wouldn': 32,\n",
       " 'shall': 32,\n",
       " 'band': 32,\n",
       " 'blue': 32,\n",
       " 'deserv': 32,\n",
       " 'myspac': 32,\n",
       " 'disappoint': 32,\n",
       " 'notic': 32,\n",
       " 'bodi': 32,\n",
       " 'club': 32,\n",
       " 'exactli': 32,\n",
       " 'green': 31,\n",
       " 'such': 31,\n",
       " 'issu': 31,\n",
       " 'lie': 31,\n",
       " 'fit': 31,\n",
       " 'bro': 31,\n",
       " 'mile': 31,\n",
       " 'cd': 31,\n",
       " 'gorgeou': 31,\n",
       " 'paper': 31,\n",
       " 'confus': 31,\n",
       " 'arm': 31,\n",
       " 'terribl': 31,\n",
       " 'ear': 31,\n",
       " 'da': 31,\n",
       " 'tan': 31,\n",
       " 'except': 31,\n",
       " 'sell': 31,\n",
       " 'twitpic': 31,\n",
       " 'fire': 31,\n",
       " 'where': 30,\n",
       " 'bu': 30,\n",
       " 'woo': 30,\n",
       " 'myself': 30,\n",
       " 'doesnt': 30,\n",
       " 'fair': 30,\n",
       " 'sadli': 30,\n",
       " 'mate': 30,\n",
       " 'project': 30,\n",
       " 'app': 30,\n",
       " 'pop': 30,\n",
       " 'countri': 30,\n",
       " 'wit': 30,\n",
       " 'uk': 30,\n",
       " 'longer': 30,\n",
       " 'crash': 30,\n",
       " 'raini': 30,\n",
       " 'caught': 30,\n",
       " 'special': 30,\n",
       " 'cover': 30,\n",
       " 'wednesday': 30,\n",
       " 'sim': 30,\n",
       " 'shift': 29,\n",
       " 'finger': 29,\n",
       " 'hubbi': 29,\n",
       " 'low': 29,\n",
       " 'si': 29,\n",
       " 'earlier': 29,\n",
       " 'gosh': 29,\n",
       " 'plu': 29,\n",
       " 'hangov': 29,\n",
       " 'invit': 29,\n",
       " 'ff': 29,\n",
       " 'david': 29,\n",
       " 'aren': 29,\n",
       " 'piec': 29,\n",
       " 'mail': 29,\n",
       " 'sale': 29,\n",
       " 'hun': 29,\n",
       " 'return': 29,\n",
       " 'ju': 28,\n",
       " 'fight': 28,\n",
       " 'both': 28,\n",
       " 'yup': 28,\n",
       " 'tom': 28,\n",
       " 'lame': 28,\n",
       " 'gave': 28,\n",
       " 'avail': 28,\n",
       " 'ahead': 28,\n",
       " 'connect': 28,\n",
       " 'act': 28,\n",
       " 'nearli': 28,\n",
       " 'fish': 28,\n",
       " 'proud': 28,\n",
       " 'view': 28,\n",
       " 'isnt': 28,\n",
       " 'prepar': 28,\n",
       " 'watchin': 28,\n",
       " 'luv': 27,\n",
       " 'yo': 27,\n",
       " 'sort': 27,\n",
       " 'interview': 27,\n",
       " 'blackberri': 27,\n",
       " 'grow': 27,\n",
       " 'recommend': 27,\n",
       " 'inde': 27,\n",
       " 'argh': 27,\n",
       " 'guitar': 27,\n",
       " 'small': 27,\n",
       " 'congratul': 27,\n",
       " 'front': 27,\n",
       " 'design': 27,\n",
       " 'offer': 27,\n",
       " 'forev': 27,\n",
       " 'thx': 27,\n",
       " 'fact': 27,\n",
       " 'cloth': 27,\n",
       " 'ma': 27,\n",
       " 'upset': 26,\n",
       " 'joy': 26,\n",
       " 'gettin': 26,\n",
       " 'white': 26,\n",
       " 'bird': 26,\n",
       " 'promis': 26,\n",
       " 'radio': 26,\n",
       " 'tune': 26,\n",
       " 'tuesday': 26,\n",
       " 'matter': 26,\n",
       " 'fill': 26,\n",
       " 'exhaust': 26,\n",
       " 'sexi': 26,\n",
       " 'self': 26,\n",
       " 'nose': 26,\n",
       " 'while': 26,\n",
       " 'deal': 26,\n",
       " 'lone': 26,\n",
       " 'version': 26,\n",
       " 'breath': 26,\n",
       " 'product': 26,\n",
       " 'gig': 26,\n",
       " 'teeth': 26,\n",
       " 'blah': 26,\n",
       " 'quiet': 26,\n",
       " 'arriv': 26,\n",
       " 'drunk': 26,\n",
       " 'storm': 26,\n",
       " 'fever': 26,\n",
       " 'singl': 26,\n",
       " 'mix': 25,\n",
       " 'de': 25,\n",
       " 'folk': 25,\n",
       " 'glass': 25,\n",
       " 'ring': 25,\n",
       " 'met': 25,\n",
       " 'pink': 25,\n",
       " 'goodby': 25,\n",
       " 'fav': 25,\n",
       " 'everybodi': 25,\n",
       " 'experi': 25,\n",
       " 'quick': 25,\n",
       " 'import': 25,\n",
       " 'child': 25,\n",
       " 'although': 25,\n",
       " 'memori': 25,\n",
       " 'daughter': 25,\n",
       " 'bag': 25,\n",
       " 'nah': 25,\n",
       " 'aint': 25,\n",
       " 'fantast': 25,\n",
       " 'behind': 25,\n",
       " 'bar': 25,\n",
       " 'three': 25,\n",
       " 'buddi': 25,\n",
       " 'huh': 25,\n",
       " 'hospit': 25,\n",
       " 'chees': 25,\n",
       " 'futur': 25,\n",
       " 'lesson': 25,\n",
       " 'john': 25,\n",
       " 'magic': 25,\n",
       " 'remind': 25,\n",
       " 'web': 25,\n",
       " 'kiss': 25,\n",
       " 'releas': 25,\n",
       " 'bday': 25,\n",
       " 'land': 25,\n",
       " 'appreci': 25,\n",
       " 'seat': 24,\n",
       " 'lake': 24,\n",
       " 'daddi': 24,\n",
       " 'ouch': 24,\n",
       " 'piss': 24,\n",
       " ...}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bagwords(text):\n",
    "    lst=[]\n",
    "    for i in text:\n",
    "        for e in i:\n",
    "            lst.append(e)\n",
    "    cfdist = ConditionalFreqDist()\n",
    "    dct={}\n",
    "    for word in lst:\n",
    "        condition = len(word)\n",
    "        cfdist[condition][word] += 1\n",
    "        dct[word]=cfdist[condition][word]\n",
    "    bag_of_words={} \n",
    "    for w in sorted(dct, key=dct.get, reverse=True)[:5000]:\n",
    "        bag_of_words[w]=dct[w]\n",
    "    return bag_of_words\n",
    "words_freq=bagwords(dfsam['text_processed'])\n",
    "words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "all_words = []\n",
    "\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "word_features = list(all_words.keys())[:3000]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
